TITLE:: GestureRT
summary:: An UGen for GRT (Gesture Recognition Toolkit) pipelines
categories:: Machine Learning

DESCRIPTION::
The UGen uses predefined pipelines and datasets, which can be generated in the GRT UI application. See http://www.nickgillian.com/wiki/pmwiki.php/GRT/GestureRecognitionToolkit for more info about GRT.

The workflow: 

NUMBEREDLIST::
## Get sensor data into the GRT UI application (downloadable from https://github.com/nickgillian/grt/releases, current version 0.2.4)
## Set up and train the model, save as pipeline
## Make a SynthDef including this UGen, and add it to the global link::Classes/SynthDescLib::
## Play synth
## Use the loadPipeline class method to load the pipeline file saved in (2)
::

CLASSMETHODS::

METHOD:: loadPipeline
Load previously saved pipeline from file

ARGUMENT:: path
Path to pipeline file

ARGUMENT:: synth
The synth object where the UGen is played

ARGUMENT:: server
The server where the UGen is played (default is Server.default)

METHOD:: loadDataset
Load and train previously saved dataset from file

NOTE:: It's probably better to use the link::#loadPipeline:: method. ::

ARGUMENT:: path
Path to dataset file

ARGUMENT:: synth
The synth object where the UGen is played

ARGUMENT:: server
The server where the UGen is played (default is Server.default)

EXAMPLES::

code::
(
SynthDef(\gestureTest, {
        //3 inputs. This could be your sensor!
        var in = { LFNoise2.kr(1).range(0,1) } ! 3;
        var g = GestureRT.kr(in);
        //1 output
        g.poll;

        //Assuming output range is 0-1, change frequency of oscillator
        Out.ar(0, SinOsc.ar(g.linexp(0, 1, 220, 440)) * 0.1.dup);
}).add;

~synth = Synth(\gestureTest); //Outputs zero before loading pipeline
)

//Load pipeline from file
GestureRT.loadPipeline("/Users/johannes/kod/supercollider/grtplugin/GRT_V0.1_18_OSX/pipeline.txt", ~synth)


//Stop
~synth.free;

::
